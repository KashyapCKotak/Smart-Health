these inputs have some weights associated to it.And the weights actually tell us how important the input is for that particular neuron.It's going to do the dot product of them,it's going to add the bias, it's going to wrap that and pass it into what we call an activation function, so this is the function which is going to give us an output of the neuron.
An activaction function is used to increase the effiency of the computation, so more complex the activation function,the better. Thus we use the sigmoid function, which gets a little bit more complicated because instead of having a linear activation, it covers the cases when you don't want your neuron to output in zero and one, but you would want it to output something in between zeros and ones, and this is where the sigmoid neuron comes into play.
